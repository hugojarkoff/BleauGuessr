{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Loss Playground\n",
    "\n",
    "Notebook to play / test tfa.losses.TripletSemiHardLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False\n",
    "if COLAB : \n",
    "    !pip install tensorflow-addons\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from plotly import express as px\n",
    "import cv2\n",
    "\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Lambda\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow_addons').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB : \n",
    "    DATA_PATH = '/content/gdrive/MyDrive/Projects/BleauGuessr/data/fulldata/'\n",
    "else :\n",
    "    DATA_PATH = '../data/fulldata/'\n",
    "\n",
    "folders = next(os.walk(DATA_PATH))[1]  # Get a list of all subdirectories\n",
    "NUM_CLASSES = len(folders)  # Count the number of subdirectories\n",
    "\n",
    "IMG_SIZE = (248, 248)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_test = image_dataset_from_directory(\n",
    "                    DATA_PATH,\n",
    "                    labels='inferred',\n",
    "                    label_mode='int',\n",
    "                    class_names=None,\n",
    "                    color_mode='rgb',\n",
    "                    batch_size=18,\n",
    "                    image_size=IMG_SIZE,\n",
    "                    shuffle=True,\n",
    "                    seed=0,\n",
    "                    subset='both',\n",
    "                    validation_split=0.3,\n",
    "                    interpolation='bilinear',\n",
    "                    follow_links=False,\n",
    "                    crop_to_aspect_ratio=True,\n",
    "                )\n",
    "\n",
    "class_names = dataset_train.class_names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize / Augment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the preprocessing function\n",
    "def _normalize_img(img, label):\n",
    "    img = tf.cast(img, tf.float32) / 255.\n",
    "    return (img, label)\n",
    "\n",
    "\n",
    "# Define the preprocessing layers for data augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.RandomRotation(0.1), \n",
    "  tf.keras.layers.RandomZoom(0.2, 0.5),\n",
    "  tf.keras.layers.RandomContrast(0.4),\n",
    "  tf.keras.layers.RandomBrightness(0.4, value_range=(0,1)),\n",
    "])\n",
    "\n",
    "# Normalize the images\n",
    "dataset_train = dataset_train.map(_normalize_img)\n",
    "dataset_test = dataset_test.map(_normalize_img)\n",
    "\n",
    "# Augmentations only on the training set\n",
    "dataset_train = dataset_train.map(lambda x, y: (data_augmentation(x), y))\n",
    "dataset_train = dataset_train.prefetch(tf.data.AUTOTUNE) # Speeds up training by caching augmentations and dataset in memory\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex in dataset_train : \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.imshow(ex[0][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model\n",
    "\n",
    "Either use model pretrained on imagenet and train only top of network, or define your own Sequential model. Not sure at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base ResNet model\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=IMG_SIZE +(3,))\n",
    "\n",
    "# Add top embedding layer\n",
    "x = base_model.output\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(128, \n",
    "                 activation='relu', \n",
    "                 )(x)\n",
    "embedding = layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1))(x) # L2 Normalize embeddings\n",
    "model = Model(inputs=base_model.input, outputs=embedding)\n",
    "\n",
    "\n",
    "# Freeze all but last conv layer of base model\n",
    "for layer in base_model.layers : \n",
    "    if layer.name not in ['conv5_block3_3_conv'] : \n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=IMG_SIZE +(3,)),\n",
    "#     tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D(pool_size=2),\n",
    "#     tf.keras.layers.Dropout(0.3),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(256, activation=None), # No activation on final dense layer\n",
    "#     tf.keras.layers.Lambda(lambda x: tf.math.l2_normalize(x, axis=1)) # L2 normalize embeddings\n",
    "# ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE : learning_rate and margin are hyperparameters to tune.\n",
    "\n",
    "\n",
    "See https://stackoverflow.com/questions/65579247/how-does-the-tensorflows-tripletsemihardloss-and-triplethardloss-and-how-to-use for more info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
    "              loss=tfa.losses.TripletSemiHardLoss(margin=0.5),\n",
    "              )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(dataset_train, epochs=15, validation_data=dataset_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, weights / losses become nan during training. Not sure why ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-2].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
